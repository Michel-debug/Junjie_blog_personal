#  🍺 深度学习调参技巧 9.26 ....🍺
作者：时间旅客
链接：https://www.zhihu.com/question/25097993/answer/3410497378
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

不管什么模型，先在一个较小的训练集上train和test，看看它能不能过拟合。如果不能过拟合，可能是学习率太大，或者代码写错了。先调小学习率试一下，如果还不行就去检查代码，先看dataloader输出的数据对不对，再看模型每一步的size是否符合自己期待。看train/eval的loss曲线，正常的情况应该是train loss呈log状一直下降最后趋于稳定，eval loss开始时一直下降到某一个epoch之后开始趋于稳定或开始上升，这时候可以用early stopping保存eval loss最低的那个模型。如果loss曲线非常不正常，很有可能是数据处理出了问题，比如label对应错了，回去检查代码。不要一开始就用大数据集，先在一个大概2w训练集，2k测试集的小数据集上调参。尽量不要自己从头搭架子（新手和半新手）。找一个已经明确没有bug能跑通的其它任务的架子，在它的基础上修改。否则debug过程非常艰难，因为有时候是版本迭代产生的问题，修改起来很麻烦。优化器优先用adam，学习率设1e-3或1e-4，再试Radam（LiyuanLucasLiu/RAdam）。不推荐sgdm，因为很慢。lrscheduler用torch.optim.lr_scheduler.CosineAnnealingLR，T_max设32或64，几个任务上试效果都不错。（用这个lr_scheduler加上adam系的optimizer基本就不用怎么调学习率了）有一些任务（尤其是有RNN的）要做梯度裁剪，torch.nn.utils.clip_grad_norm。参数初始化，lstm的h用orthogonal，其它用he或xavier。激活函数用relu一般就够了，也可以试试leaky relu。batchnorm和dropout可以试，放的位置很重要。优先尝试放在最后输出层之前，以及embedding层之后。RNN可以试layer_norm。有些任务上加了这些层可能会有负作用。metric learning中先试标label的分类方法。然后可以用triplet loss，margin这个参数的设置很重要。batchsize设置小一点通常会有一些提升，某些任务batchsize设成1有奇效。embedding层的embedsize可以小一些（64 or 128），之后LSTM或CNN的hiddensize要稍微大一些（256 or 512）。（ALBERT论文里面大概也是这个意思）模型方面，可以先用2或3层LSTM试一下，通常效果都不错。weight decay可以试一下，我一般用1e-4。有CNN的地方就用shortcut。CNN层数加到某一个值之后对结果影响就不大了，这个值作为参数可以调一下。GRU和LSTM在大部分任务上效果差不多。看论文时候不要全信，能复现的尽量复现一下，许多论文都会做低baseline，但实际使用时很多baseline效果很不错。对于大多数任务，数据比模型重要。面对新任务时先分析数据，再根据数据设计模型，并决定各个参数。例如nlp有些任务中的padding长度，通常需要达到数据集的90%以上，可用pandas的describe函数进行分析。