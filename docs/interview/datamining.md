---
title: 集成算法
author: junjie
date: '2024-10-07'
---
# 集成算法
## 随机森林算法 (bagging)
note: 通过组合多个弱分类器，最终结果通过投票或取均值，使得整体模型的结果具有较高的精确度和泛化性能，同时也有很好的稳定性，广泛应用在各种业务场景中。 
![alt text](./datamining.assets/randomtree.png)
- 由于随机森林引入了**样本扰动**和**特征扰动**，从而很大程度上提高了模型的泛化能力，尽可能地避免了过拟合现象的出现。 样本扰动具体表现在作为bagging的集成算法，对于总体样本集T，抽样一个子集作为训练样本集，同时对于k个特征，每次仅选择d < k 个特征构建决策树。 达到了样本扰动和特征扰动的效果。
- 随机森林可以处理高维数据，无需进行**特征选择**，在训练过程中可以得出不同特征对模型的重要性程度。
- 随机森林的每个基分类器采用决策树，方法简单且容易实现。同时每个基分类器之间没有相互依赖关系，整个算法易并行化.  
**Note**:  
随机森林不需要对特征进行规范化，因为他是基于决策树的集成算法，决策树的工作原理并不依赖于特征值的尺度或范围。通过特征来分割规则，使用阈值来判断，并不是通过计算距离来判断，特征的绝对尺度对数的分裂点选择没有影响。树仅关心某个特质是否大于或小于特定的阈值。 对特征分布不敏感，决策树的构建过程是对每个特征进行独立分裂，只涉及特征值的排序和阈值的选择，不依赖于数据是否规范化，无论特征是否被缩放，树的分裂方法，最终结果不会收到影响。 
不像svm， knn 等基于距离或内积计算的模型，对特征尺度较为敏感

## Boosting
Boosting 是一种提升算法，可以将弱的学习算法提升 (boost) 为强的学习算法。基本思路如下：

1. 利用初始训练样本集训练得到一个基学习器。
2. 提高被基学习器误分的样本的权重，使得那些被错误分类的样本在下一轮训练中可以得到更大的关注，利用调整后的样本训练得到下一个基学习器。
3. 重复上述步骤，直至得到个学习器。
4. 对于分类问题，采用有权重的投票方式；对于回归问题，采用加权平均得到预测值。

## GBDT 梯度提升树 (回归问题，分类问题都可)
简单来说, 就是先对目标值求平均，然后算出每一组目标值的残差是多少，通过拟合残差的形式，$f_2(x) = f_1(x) + \alpha * T(x)$, 不断循环直到平方误差小于某一个阈值为止。这里的alpha是学习率(控制每棵树对整体模型的贡献)，通常取0.5，在[0,1]之间即可，T(x)为每一次的残差。(损失函数是最小均方误差)  
对于二分类问题，残差计算使用损失函数的梯度，结果输出使用sigmod函数，多分类问题使用softmax函数。<color font='red'>之所以使用梯度，因为负梯度就是残差的近似值</color> 使用的是目标分类的概率值，与预测概率之间的差值进行迭代  
![alt text](./datamining.assets/image.png)  
课件:<embed src="/GBDT_cours.pdf" type="application/pdf" width="100%" height="600px" />
<div style="text-align: center">*以上资源来自网上共享，如有侵权请联系我删除*</div>  
  
提升树和gbdt的区别主要在哪里：
-  提升树的概念（Boosted Trees） 
提升树本质上是一种集成方法，它通过迭代训练一系列决策树，每一棵树都试图纠正前一棵树的错误。基本思想是将多个简单模型（弱学习器）组合在一起，最终获得一个强大的模型。 

Boosting 的概念早在 AdaBoost 算法中引入，后来的许多改进都基于这一思想。提升树（Boosted Trees）是 Boosting 和决策树结合的一个分支，它可以有多种实现方式。 

- GBDT 是梯度提升（Gradient Boosting）的特定实现  
GBDT 是提升树的一个具体实现，它使用梯度下降的思想来优化模型。GBDT 每一步都基于模型的残差（即当前模型与实际目标之间的差异），并通过拟合新的决策树来逐步减少这些残差。 

GBDT 的主要特点：
损失函数最小化：GBDT 在每一轮迭代中通过最小化损失函数来更新模型。损失函数可以是均方误差、对数损失函数（分类任务）等。
梯度下降优化：通过计算损失函数的梯度来决定下一棵树如何拟合当前的残差，逐步逼近目标。
用于回归和分类：GBDT 可以用于回归和分类任务，常用的损失函数包括均方误差（回归）和对数损失（分类）。
![alt text](./datamining.assets/gbdt.png)


## XGBOOST算法
*课件*  
<embed src="/xgboost.pdf" type="application/pdf" width="100%" height="600px" />
<div style="text-align: center">*以上资源来自网上共享，如有侵权请联系我删除*</div>  
笔记:  
还得看 xgboost 数学推导公式 有点难，看不懂反复看别人的教程。

